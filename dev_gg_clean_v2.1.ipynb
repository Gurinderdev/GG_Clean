{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note\n",
    "Please note that this code is NOT optimised and initialy there is no attempt to optimise. \n",
    "There is no attempt to fine tune models at this stage \n",
    "The goal is to get a test multiple hypothesis and see what best concepts can be build in. \n",
    "\n",
    "# Result Summary:\n",
    "Accuracy achieved 97% on 8 possible targets\n",
    "Accuracy achieved : 84% on 315 possible targets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-DO\n",
    "1) compute words which have 70% similar characters\n",
    "\n",
    "2) create a number based represenation of the words in the target. this will help the random forest to distinguish the target names\n",
    "\n",
    "3) use the represenation to come up with different metrics for the words with similar characters\n",
    "\n",
    "4) Build the dictionary for key probability -- pending\n",
    "\n",
    "5) build the similarity / probability score for each word match or not -- pending\n",
    "\n",
    "6) use the number represenation to add uniqueness to each word  -- pending"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\GG_Clean\n"
     ]
    }
   ],
   "source": [
    "cd D:\\GG_Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pickle\n",
    "import collections \n",
    "from collections import Counter \n",
    "import string\n",
    "\n",
    "from random import shuffle # this helps in shuffling numbers randomly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"file_for_github.xlsx\") # file is available on the github page.... incase you wish to run on your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_name</th>\n",
       "      <th>final_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COMPUTERSHARE TRUST COMPANY OF CANADA</td>\n",
       "      <td>COMPUTERSHARE TRUST COMPANY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>\"COMPUTERSHARE TRUST COMPANY OF CANADA\" (THE \"...</td>\n",
       "      <td>COMPUTERSHARE TRUST COMPANY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>* NEW PARTY COMPUTERSHARE TRUST COMPANY OF CANADA</td>\n",
       "      <td>COMPUTERSHARE TRUST COMPANY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>* NEW PARTY COMPUTERSHARE TRUST COMPANY OF CAN...</td>\n",
       "      <td>COMPUTERSHARE TRUST COMPANY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>*COMPUTERSHARE TRUST COMPANY OF CANADA</td>\n",
       "      <td>COMPUTERSHARE TRUST COMPANY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_name  \\\n",
       "0             Â COMPUTERSHARE TRUST COMPANY OF CANADA   \n",
       "1  \"COMPUTERSHARE TRUST COMPANY OF CANADA\" (THE \"...   \n",
       "2  * NEW PARTY COMPUTERSHARE TRUST COMPANY OF CANADA   \n",
       "3  * NEW PARTY COMPUTERSHARE TRUST COMPANY OF CAN...   \n",
       "4             *COMPUTERSHARE TRUST COMPANY OF CANADA   \n",
       "\n",
       "                    final_name  \n",
       "0  COMPUTERSHARE TRUST COMPANY  \n",
       "1  COMPUTERSHARE TRUST COMPANY  \n",
       "2  COMPUTERSHARE TRUST COMPANY  \n",
       "3  COMPUTERSHARE TRUST COMPANY  \n",
       "4  COMPUTERSHARE TRUST COMPANY  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "here the user will provide a train data-set with pre-cleaned data from input_name to final_name. \n",
    "\n",
    "once the model is trained it can be used to clean other similar such stuctures. \n",
    "\n",
    "In this data set there are 8 uniuque elements in  final_name. this is our target variable.\n",
    "\n",
    "The input_name is the raw input. it can vary dramaticly in number of characters, number of words, the very charectors themselves. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## to do \n",
    "Create features that meet the following requirements \n",
    "\n",
    "1) Words with complete match \n",
    "\n",
    "2) words with 70% match\n",
    "\n",
    "3) Words with complete match but weighted based on a number for the target name\n",
    "\n",
    "4) Words with 70% match but weighted based on a number for the target name"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Psudo Code\n",
    "\n",
    "for each index \n",
    "  what is the max value in that row\n",
    "        if that value is 1 \n",
    "            then it goes into word_100\n",
    "            else it goes into word_70 if it meats a cut-off\n",
    "            else it is discarded\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create , fill , and read a dataframe for each row to get values for \n",
    "exceptions_captured = []\n",
    "for main_index in data.index:\n",
    "    try:\n",
    "        cut_off_percentage = 0.7\n",
    "        word_100 = 0\n",
    "        word_75 = 0\n",
    "\n",
    "        df_word_similarity = pd.DataFrame(columns=[ str(i) for i in data.loc[main_index , 'input_name'].split()]\n",
    "                                          , index= [str(j) for j in data.loc[main_index , 'final_name'].split()])\n",
    "\n",
    "        # fill in the dataframe \n",
    "        for i in data.loc[main_index , 'input_name'].split():\n",
    "            counter_i = Counter(i)\n",
    "\n",
    "            for j in data.loc[main_index , 'final_name'].split():\n",
    "                counter_j = Counter(j)\n",
    "                diff_in_counters = counter_i & counter_j\n",
    "\n",
    "                overlap_i_j = float(sum(diff_in_counters.values()) / sum(counter_i.values()))\n",
    "                df_word_similarity.loc[i,j] = overlap_i_j\n",
    "\n",
    "        # read from the dataframe to get the word_100 and word_75\n",
    "\n",
    "        for i in df_word_similarity.index:\n",
    "            y = pd.Series(df_word_similarity.loc[str(i) , :]).astype('float64') # the float is done so that we can get y.idxmax()\n",
    "            if y.max() == 1 :\n",
    "                word_100 += 1\n",
    "            elif y.max() >= cut_off_percentage:\n",
    "                word_75 += 1\n",
    "            # for later when we need the name of the word we can do y.idxmax() to get the name of the column.. \n",
    "            #########################\n",
    "            # here we need to add the weights of the target word to the data \n",
    "            # this will help to get the model to focus on the target word\n",
    "            #########################\n",
    "\n",
    "        # update the main dataframe \n",
    "\n",
    "        data.loc[main_index , 'word_100'] = word_100\n",
    "        data.loc[main_index , 'word_75']  = word_75  # later we can change word_75 to word_cut_off\n",
    "    except KeyboardInterrupt:\n",
    "        raise\n",
    "\n",
    "    except Exception as e:\n",
    "        exceptions_captured.append(main_index)\n",
    "        pass\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate new features based on  number calculation \n",
    "\n",
    "\n",
    "Get 2 new features from the earlier 2 features by normalisation and multiplication by the number for each target group o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the  number for each final_name ... for example Td Bank = 300\n",
    "\n",
    "unique_final_names = list(data.final_name.unique())\n",
    "shuffle(unique_final_names) # here inplace is true by default \n",
    "                          \n",
    "dict_final_word_prime_number = {}\n",
    "\n",
    "for i , j in zip(unique_final_names ,range(1, 100000000000 , 300 )) :\n",
    "    \n",
    "    dict_final_word_prime_number[i] = j\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 2 new features - based on normalisation \n",
    "\n",
    "data['word_100_normalised'] = \"\"\n",
    "data[\"word_75_normalised\"] = \"\"\n",
    "\n",
    "for i in data.index:\n",
    "    len_input_name = len(data.loc[i , 'input_name'].split())\n",
    "    len_input_name = [len_input_name if len_input_name != 0 else 1]\n",
    "    \n",
    "    data.loc[i, 'word_100_normalised'] = data.loc[i,'word_100']/len_input_name\n",
    "    data.loc[i, 'word_75_normalised'] = data.loc[i,'word_75']/len_input_name\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate 2 new features with the weightage of  numbers for each Fianl name .... This will help to individualize the \n",
    "# data in each final_name \n",
    "\n",
    "data['word_100_prime'] = \"\"\n",
    "data[\"word_75_prime\"] = \"\"\n",
    "data['word_100_normalised_prime'] = \"\"\n",
    "data[\"word_75_normalised_prime\"] = \"\"\n",
    "\n",
    "for i in data.index:\n",
    "   \n",
    "    y = dict_final_word_prime_number.get(data.loc[i, 'final_name'])\n",
    "    \n",
    "    data.loc[i, 'word_100_prime'] = data.loc[i,'word_100']*y\n",
    "    data.loc[i, 'word_75_prime'] = data.loc[i,'word_75']*y\n",
    "    data.loc[i, 'word_100_normalised_prime'] = data.loc[i,'word_100_normalised']*y\n",
    "    data.loc[i, 'word_75_normalised_prime'] = data.loc[i,'word_75_normalised']*y\n",
    "    \n",
    "data.fillna(value= 0 , inplace= True)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Model \n",
    "\n",
    "Here i am using a Random Forest. There is no attempt to fine-tune. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dependencies\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "def metrics(y_test , y_pred):\n",
    "    print('accuracy_score is {}'.format(accuracy_score(y_test , y_pred)))\n",
    "    print('\\n confusion_matrix is \\n {}'.format(confusion_matrix(y_test,y_pred)))\n",
    "    print('\\n classification report is \\n {}'.format(classification_report(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  model development \n",
    "\n",
    "features= [\n",
    " 'word_100',\n",
    " 'word_75',\n",
    " 'word_100_normalised',\n",
    " 'word_75_normalised',\n",
    " 'word_100_prime',\n",
    " 'word_75_prime',\n",
    " 'word_100_normalised_prime',\n",
    " 'word_75_normalised_prime'\n",
    "    ]\n",
    "\n",
    "target = ['final_name']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data[features], data[target], test_size=0.33, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate an instance of the model \n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "preds = rf.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy_score is 0.9632352941176471\n",
      "\n",
      " confusion_matrix is \n",
      " [[151   2   0   0   0   0   0   0]\n",
      " [  2 283   0   0   0   0   0   0]\n",
      " [  0   1  14   1   0   0   0   2]\n",
      " [  0   0   2   3   0   0   0   0]\n",
      " [  1   0   0   0   0   0   0   0]\n",
      " [  0   0   0   0   0 197   0   0]\n",
      " [  0   0   0   0   0   0   2   0]\n",
      " [  0  11   3   0   0   0   0   5]]\n",
      "\n",
      " classification report is \n",
      "                                     precision    recall  f1-score   support\n",
      "\n",
      "       COMPUTERSHARE TRUST COMPANY       0.98      0.99      0.98       153\n",
      "                           TD BANK       0.95      0.99      0.97       285\n",
      "                   TD CANADA TRUST       0.74      0.78      0.76        18\n",
      "   TD FINANCIAL SERVICES HOME INC.       0.75      0.60      0.67         5\n",
      "TD HOME AND AUTO INSURANCE COMPANY       0.00      0.00      0.00         1\n",
      "                  TD TRUST COMPANY       1.00      1.00      1.00       197\n",
      "                     TD WATERHOUSE       1.00      1.00      1.00         2\n",
      "             TORONTO DOMINION BANK       0.71      0.26      0.38        19\n",
      "\n",
      "                         micro avg       0.96      0.96      0.96       680\n",
      "                         macro avg       0.77      0.70      0.72       680\n",
      "                      weighted avg       0.96      0.96      0.96       680\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics(y_test ,preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = pd.DataFrame(rf.feature_importances_,\n",
    "                                   index = X_train.columns,\n",
    "                                    columns=['importance']).sort_values('importance',  ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>word_100</th>\n",
       "      <td>0.257767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_100_normalised_prime</th>\n",
       "      <td>0.226924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_100_prime</th>\n",
       "      <td>0.221265</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_75_normalised_prime</th>\n",
       "      <td>0.088959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_75_prime</th>\n",
       "      <td>0.088607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_100_normalised</th>\n",
       "      <td>0.058772</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_75_normalised</th>\n",
       "      <td>0.047631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word_75</th>\n",
       "      <td>0.010075</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           importance\n",
       "word_100                     0.257767\n",
       "word_100_normalised_prime    0.226924\n",
       "word_100_prime               0.221265\n",
       "word_75_normalised_prime     0.088959\n",
       "word_75_prime                0.088607\n",
       "word_100_normalised          0.058772\n",
       "word_75_normalised           0.047631\n",
       "word_75                      0.010075"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL\n",
    "import pydotplus\n",
    "import warnings\n",
    "from glob import glob\n",
    "from IPython.display import display, Image\n",
    "# from sklearn.datasets import load_iris\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import export_graphviz\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# source credit: https://www.youtube.com/watch?v=SMGEmCOUBUw&t=81s \n",
    "\n",
    "def save_decision_trees_as_png(clf, iteration, feature_name, target_name):\n",
    "    file_name = \"tree\" + str(iteration) + \".png\"\n",
    "    dot_data = export_graphviz(\n",
    "        clf,\n",
    "        out_file=None,\n",
    "        feature_names=feature_name,\n",
    "        class_names=target_name,\n",
    "        rounded=True,\n",
    "        proportion=False,\n",
    "        precision=2,\n",
    "        filled=True,\n",
    "        )\n",
    "    graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "    graph.write_png(file_name)\n",
    "    print(\"Decision Tree {} saved as png file\".format(iteration + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree 1 saved as png file\n",
      "Decision Tree 2 saved as png file\n",
      "Decision Tree 3 saved as png file\n",
      "Decision Tree 4 saved as png file\n"
     ]
    }
   ],
   "source": [
    "feature_names = features\n",
    "target_names = data['final_name'].unique()\n",
    "\n",
    "for i in range(len(rf.estimators_)):\n",
    "    save_decision_trees_as_png(rf.estimators_[i], i, feature_names, target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [ PIL.Image.open(f) for f in glob('./*.png') ]\n",
    "\n",
    "for im in images:\n",
    "    display(Image(filename=im.filename, retina=True))\n",
    "    im.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "please click on any tree to expand the view"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
